{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJiUYjQceLAA",
        "outputId": "19bc8dcf-6ed6-4ac7-c607-c1d17188b350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ece0f2a"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Spark session create\n",
        "spark = SparkSession.builder.appName(\"BigDataAnalysis\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7e780a92",
        "outputId": "92695378-ff38-414b-bf37-0d45f783073f"
      },
      "source": [
        "# Dataset from internet (no need to download)\n",
        "url = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\"\n",
        "\n",
        "# Columns manually define karte hain\n",
        "columns = [\"AirportID\", \"Name\", \"City\", \"Country\", \"IATA\", \"ICAO\",\n",
        "           \"Latitude\", \"Longitude\", \"Altitude\", \"Timezone\", \"DST\", \"Tz\", \"Type\", \"Source\"]\n",
        "\n",
        "# CSV read karna\n",
        "df = spark.read.csv(url, header=False, inferSchema=True)\n",
        "df = df.toDF(*columns)\n",
        "\n",
        "# Top 5 rows dekhte hain\n",
        "df.show(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnsupportedOperationException",
          "evalue": "None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-377578670.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# CSV read karna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedOperationException\u001b[0m: None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddd507fa"
      },
      "source": [
        "# Task\n",
        "Modify the empty code cell to download the CSV file from \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" to a local temporary file, load it into a Spark DataFrame, rename the columns to \"AirportID\", \"Name\", \"City\", \"Country\", \"IATA\", \"ICAO\", \"Latitude\", \"Longitude\", \"Altitude\", \"Timezone\", \"DST\", \"Tz\", \"Type\", and \"Source\", and display the first 5 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41987adb"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "### Subtask:\n",
        "Download the CSV file from the URL to a local temporary file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "664c7430"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because PySpark cannot directly read from a URL. I need to download the file first to a local temporary file before loading it into a Spark DataFrame. This step focuses on downloading the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc1f042b",
        "outputId": "26f1bcdd-1cf7-4e29-a50c-5279e9a3172e"
      },
      "source": [
        "import requests\n",
        "import tempfile\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\"\n",
        "\n",
        "# Create a temporary file\n",
        "temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "\n",
        "# Download the content\n",
        "response = requests.get(url)\n",
        "\n",
        "# Write the content to the temporary file\n",
        "with open(temp_file.name, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# The temporary file path is stored in temp_file.name\n",
        "print(f\"Downloaded file to: {temp_file.name}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded file to: /tmp/tmpayak1kcw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13259832"
      },
      "source": [
        "**Reasoning**:\n",
        "I have successfully downloaded the file to a temporary location. The next step is to load this temporary file into a Spark DataFrame and perform the requested operations. This completes the overall subtask of downloading and preparing the data in Spark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51e4a422",
        "outputId": "31d4611d-abd6-4e5d-8bd6-ac325da4ffa1"
      },
      "source": [
        "# Columns manually define karte hain\n",
        "columns = [\"AirportID\", \"Name\", \"City\", \"Country\", \"IATA\", \"ICAO\",\n",
        "           \"Latitude\", \"Longitude\", \"Altitude\", \"Timezone\", \"DST\", \"Tz\", \"Type\", \"Source\"]\n",
        "\n",
        "# CSV read karna from the temporary file\n",
        "df = spark.read.csv(temp_file.name, header=False, inferSchema=True)\n",
        "df = df.toDF(*columns)\n",
        "\n",
        "# Top 5 rows dekhte hain\n",
        "df.show(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+------------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
            "|AirportID|                Name|        City|         Country|IATA|ICAO|          Latitude|         Longitude|Altitude|Timezone|DST|                  Tz|   Type|     Source|\n",
            "+---------+--------------------+------------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
            "|        1|      Goroka Airport|      Goroka|Papua New Guinea| GKA|AYGA|-6.081689834590001|     145.391998291|    5282|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        2|      Madang Airport|      Madang|Papua New Guinea| MAG|AYMD|    -5.20707988739|     145.789001465|      20|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        3|Mount Hagen Kagam...| Mount Hagen|Papua New Guinea| HGU|AYMH|-5.826789855957031|144.29600524902344|    5388|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        4|      Nadzab Airport|      Nadzab|Papua New Guinea| LAE|AYNZ|         -6.569803|        146.725977|     239|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        5|Port Moresby Jack...|Port Moresby|Papua New Guinea| POM|AYPY|-9.443380355834961|147.22000122070312|     146|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "+---------+--------------------+------------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4f404f"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the downloaded CSV file into a Spark DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3323d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the data from the temporary file path into a Spark DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d70f500"
      },
      "source": [
        "df = spark.read.csv(temp_file.name, header=False, inferSchema=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b0af7bb"
      },
      "source": [
        "## Rename columns and display data\n",
        "\n",
        "### Subtask:\n",
        "Rename the columns of the DataFrame and display the first few rows as requested in the original code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06647ec3"
      },
      "source": [
        "**Reasoning**:\n",
        "Rename the columns of the DataFrame and display the first 5 rows as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f4f3b99",
        "outputId": "bec50425-5688-499a-c9c8-1599b4f6eb97"
      },
      "source": [
        "# Columns manually define karte hain\n",
        "columns = [\"AirportID\", \"Name\", \"City\", \"Country\", \"IATA\", \"ICAO\",\n",
        "           \"Latitude\", \"Longitude\", \"Altitude\", \"Timezone\", \"DST\", \"Tz\", \"Type\", \"Source\"]\n",
        "\n",
        "# Rename columns\n",
        "df = df.toDF(*columns)\n",
        "\n",
        "# Top 5 rows dekhte hain\n",
        "df.show(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+------------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
            "|AirportID|                Name|        City|         Country|IATA|ICAO|          Latitude|         Longitude|Altitude|Timezone|DST|                  Tz|   Type|     Source|\n",
            "+---------+--------------------+------------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
            "|        1|      Goroka Airport|      Goroka|Papua New Guinea| GKA|AYGA|-6.081689834590001|     145.391998291|    5282|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        2|      Madang Airport|      Madang|Papua New Guinea| MAG|AYMD|    -5.20707988739|     145.789001465|      20|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        3|Mount Hagen Kagam...| Mount Hagen|Papua New Guinea| HGU|AYMH|-5.826789855957031|144.29600524902344|    5388|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        4|      Nadzab Airport|      Nadzab|Papua New Guinea| LAE|AYNZ|         -6.569803|        146.725977|     239|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "|        5|Port Moresby Jack...|Port Moresby|Papua New Guinea| POM|AYPY|-9.443380355834961|147.22000122070312|     146|      10|  U|Pacific/Port_Moresby|airport|OurAirports|\n",
            "+---------+--------------------+------------+----------------+----+----+------------------+------------------+--------+--------+---+--------------------+-------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98273a1c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The CSV file from the provided URL was successfully downloaded to a local temporary file.\n",
        "*   The data was loaded into a Spark DataFrame without a header, and the schema was inferred.\n",
        "*   The columns of the DataFrame were successfully renamed to \"AirportID\", \"Name\", \"City\", \"Country\", \"IATA\", \"ICAO\", \"Latitude\", \"Longitude\", \"Altitude\", \"Timezone\", \"DST\", \"Tz\", \"Type\", and \"Source\".\n",
        "*   The first 5 rows of the DataFrame with the new column names were displayed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The data is now prepared in a Spark DataFrame with meaningful column names for further analysis or processing.\n",
        "*   The temporary file created during the download process should be properly managed (e.g., deleted) if not needed for subsequent steps to free up disk space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac5d02ca",
        "outputId": "72028faf-8d7c-4f17-fd35-d9e1ddf7b919"
      },
      "source": [
        "print(\"Total Airports:\", df.count())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Airports: 7698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5194ac0",
        "outputId": "e0aac184-0143-4556-a9e7-13e6a15417a3"
      },
      "source": [
        "df.groupBy(\"Country\").count().orderBy(\"count\", ascending=False).show(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|       Country|count|\n",
            "+--------------+-----+\n",
            "| United States| 1512|\n",
            "|        Canada|  430|\n",
            "|     Australia|  334|\n",
            "|        Russia|  264|\n",
            "|        Brazil|  264|\n",
            "|       Germany|  249|\n",
            "|         China|  241|\n",
            "|        France|  217|\n",
            "|United Kingdom|  167|\n",
            "|         India|  148|\n",
            "+--------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6075e342",
        "outputId": "5112abc5-3977-49f3-d2ba-ec848c0a20b9"
      },
      "source": [
        "df.filter(df.Country == \"India\").select(\"Name\", \"City\").show(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|                Name|      City|\n",
            "+--------------------+----------+\n",
            "|Sardar Vallabhbha...| Ahmedabad|\n",
            "|       Akola Airport|     Akola|\n",
            "|  Aurangabad Airport|Aurangabad|\n",
            "|Chhatrapati Shiva...|    Mumbai|\n",
            "|    Bilaspur Airport|  Bilaspur|\n",
            "|        Bhuj Airport|      Bhuj|\n",
            "|     Belgaum Airport|   Belgaum|\n",
            "|    Vadodara Airport|    Baroda|\n",
            "|Raja Bhoj Interna...|    Bhopal|\n",
            "|   Bhavnagar Airport| Bhaunagar|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f81545a5",
        "outputId": "b77d8c45-ccb6-4d18-ebea-c50c17b319b7"
      },
      "source": [
        "df.filter(df.Country == \"United Kingdom\").select(\"Name\", \"City\").show(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+\n",
            "|                Name|             City|\n",
            "+--------------------+-----------------+\n",
            "|Belfast Internati...|          Belfast|\n",
            "|   St Angelo Airport|      Enniskillen|\n",
            "|George Best Belfa...|          Belfast|\n",
            "|City of Derry Air...|      Londonderry|\n",
            "|Birmingham Intern...|       Birmingham|\n",
            "|    Coventry Airport|         Coventry|\n",
            "|   Leicester Airport|        Leicester|\n",
            "|Gloucestershire A...|Golouchestershire|\n",
            "|Wolverhampton Hal...|  Halfpenny Green|\n",
            "|    Cotswold Airport|          Pailton|\n",
            "+--------------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}